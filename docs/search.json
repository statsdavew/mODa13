{
  "articles": [
    {
      "path": "history.html",
      "title": "mODa13",
      "description": "<b>History<\/b>",
      "author": [],
      "contents": "\nPast mODa workshops:\nmODa 1: Eisenach, former GDR, 1987\nmODa 2: St. Kyrik, Bulgaria, 1990\nmODa 3: Peterhof, Russia, 1992\nmODa 4: Spetses, Greece, 1995\nmODa 5: Luminy, France, 1998\nmODa 6: Puchberg/Schneeberg, Austria, 2001\nmODa 7: Heeze, The Netherlands, 2004\nmODa 8: Almagro, Spain, 2007\nmODa 9: Bertinoro, Italy, 2010\nmODa 10: Łagów Lubuski, Poland, 2013\nmODa 11: Hamminkeln-Dingden, Germany, 2016\nmODa 12: Smolenice, Solvakia, 2019\n\n\n\n",
      "last_modified": "2023-06-15T21:30:52+01:00"
    },
    {
      "path": "index.html",
      "title": "mODa13",
      "author": [],
      "contents": "\n\n\n\nThe scientific workshops on model-oriented data analysis and optimum design (mODa) provide a high-level international forum for researchers, professionals and practitioners to present and discuss recent advances, new techniques and applications in the field of optimum experimental design. In addition, a primary aim is to provide young researchers an opportunity to establish personal contacts with leading specialists in the field. Its specificity is that the participation is by invitation of the board only.\n\nmODa is (generally) organized every three years, with the previous mODa12 held in Smolenice, Slovakia. After a short Covid-induced delay, the 13th edition of the workshop series will be held in Southampton, UK, 9-14 July 2023.\nSupported by:\n\n\n\n\n\n\n",
      "last_modified": "2023-06-15T21:30:53+01:00"
    },
    {
      "path": "location.html",
      "title": "mODa13",
      "description": "<b>Location<\/b>",
      "author": [],
      "contents": "\nThe 13th mODa workshop will be hosted by the Southampton Statistical Sciences Research Institute (S3RI) at the University of Southampton Highfield Campus.\n\n\n\nThere has been an institution of higher education (the Hartley Institute) in Southampton since 1862, with a University College at the main Highfield Campus since 1919. The University gained it’s Royal Charter in 1952. S3RI was formed in 2003 to synergise the research of statisticians across the Mathematical, Social and Health Sciences, and Medicine. It will be celebrating its 20th anniversary when mODa13 is held.\nThe scientific sessions of mODa will be held in Building 100 (the Centenary Building), opened in 2019 to celebrate 100 years of higher education on the Highfield Campus. Accommodation will be in Chamberlain Halls.\n\n\n\n\n\n\n\n",
      "last_modified": "2023-06-15T21:30:53+01:00"
    },
    {
      "path": "organisation.html",
      "title": "mODa13",
      "description": "<b>Organisation<\/b>",
      "author": [],
      "contents": "\nmODa13 organising committee\nDave Woods (local organiser)\nRadoslav Harman (local organiser of mODa12)\nMartina Vandebroek (local organiser of mODa14)\nPeter Goos (local organiser of mODa14)\nmODa13 local organising committee\nSelin Ahipasaoglu\nVasiliki Koutra\nHendriico Merila\nmODa board\nHolger Dette\nPeter Goos\nRadoslav Harman\nJesús López Fidalgo\nTobias Mielke\nChristine Müller\nWerner Müller\nLuc Pronzato\nChiara Tommasi\nDariusz Uciński\nMartina Vandebroek\nDave Woods\nHenry Wynn\nAnatoly Zhigljavsky\nHonorary lifetime invitees\nAnthony Atkinson\nValerii Fedorov\nAndrej Pázman\nBen Torsney\n\n\n\n",
      "last_modified": "2023-06-15T21:30:54+01:00"
    },
    {
      "path": "proceedings.html",
      "title": "mODa13",
      "description": "<b>Proceedings<\/b>",
      "author": [],
      "contents": "\nWorkshop participants are encouraged to submit research based on their presentations for publication in the mODa proceedings, a special issue of Statistical Papers. When submitting to the journal, please select article type as “S.I. : MODA13”. The special issue is edited by Dave Woods, Stefanie Biedermann and Chiara Tommasi.\n\n\n\n",
      "last_modified": "2023-06-15T21:30:55+01:00"
    },
    {
      "path": "programme_full.html",
      "title": "mODa programme",
      "author": [],
      "contents": "\n\n\n\n\n\n\n  \n\n\n  mODa programme\n  \n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmODa13\n\n\nHome\nOrganisation\nRegistration\nLocation\nTravel\nProceedings\nTimetable\nProgramme\nHistory\n☰\n\n\n\n\n\n\n\n\n\n\nMonday\n\n\n\n\nMonday\n    10 July 2023\n    08:50 Welcome\n    \nDave Woods (University of Southampton)09:00 Discrete choice experiments (Chair: Martina Vandebroek, KU Leuven)\n    Bayesian \\(D\\)- And \\(I\\)-Optimal Designs For Choice Experiments Involving Mixtures And Process Variables\n\nMario Becerra (KU Leuven)On The Impact Of Decision Rule Assumptions In Experimental Designs On Preference Recovery\n\nSander van Cranenburgh (TU Delft)Design Replication In Partial-Profile Choice Experiments\n\nHeiko Großmann (Universität Magdeburg)Optimal Designs For Discrete Choice Models Via Graph Laplacians\n\nFrank Röttger (Université de Genève)\n10:30 Coffee break\n    \n11:00 Computational challenges in experimental design (Chair: Radoslav Harman, Comenius University Bratislava)\n    On Gaussian Process Multiple-Fold Cross-Validation\n\nDavid Ginsbourger (University of Bern)A Modified Frank–Wolfe Algorithm For The Dk-Optimal Design Problem\n\nSelin Ahipasaoglu (University of Southampton)Mixed-Integer Linear Programming For Computing Optimal Designs\n\nSamuel Rosa (Comenius University Bratislava)TBA\nTBA\n12:30 Lunch break\n    \n14:00 Subsampling, dynamic and non-linear models (Chair: TBA)\n    Optimal Subsampling Design For Polynomial Regression In One Covariate\n\nTorsten Reuter (Universität Magdeburg)Model Robust Subsampling Approach For Generalised Linear Models In Big Data Settings\n\nAmalan Mahendran (Queensland University of Technology)Adaptive And Robust Experimental Design For Linear Dynamic Models Using The Kalman Filter\n\nArno StrouwenOptimum Design For Ill-Conditioned Models: \\(K\\)–Optimality And Stable Parameterizations\n\nAnthony Atkinson (London School of Economics)\n15:30 Coffee break\n    \n16:00 Clinical trials I (Chair: Chiara Tommasi, Università degli Studi di Milano)\n    Optimal Design For Inference On The Threshold Of A Biomarker\n\nRosamarie Frieri (Università di Bologna)Posterior Alternatives With Informative Early Stopping\n\nNancy Flournoy (University of Missouri)Design And Inference For Enrichment Trials With A Continuous Biomarker\n\nBill Rosenberger (George Mason University)About \\(C\\)- And \\(D\\)-Optimal Dose-Finding Designs For Bivariate Outcomes\n\nFrank Miller (Linköping and Stockholm Universities)\n17:30 mODa 14\n    \nMartina Vandebroek & Peter Goos\n\n\nTuesday\n\n\n\n\nTuesday\n    11 July 2023\n    09:00 TBA (Chair: TBA)\n    Valid Restricted Randomization For Small Experiments\n\nRosemary Bailey (University of St Andrews)\\(D\\)-Optimal And Nearly \\(D\\)-Optimal Exact Designs For Binary Response On The Ball\n\nMartin Radloff (Universität Magdeburg)A Methodology To Augment Designs\n\nCarlos de la Calle Arroyo (Universidad de Navarra)TBA\nAndrew Hooker (Uppsala Univeritet)\n10:30 Coffee break\n    \n11:00 Optimal Sensor Location for Spatiotemporal Processes and Networks (Chair: TBA)\n    \\(A\\)-Optimal Designs For State Estimation In Networks\n\nKirsten Schorning (Technische Universität Dortmund)Optimal Experimental Design For Infinite-Dimensional Bayesian Inverse\n\nAlen Alexanderian (North Carolina State University)Problems Under Model Uncertainty\n\nKarina Koval (Universität Heidelberg)Optimal Sensor Location For Spatiotemporal Processes And Networks\n\nDariusz Uciński (University of Zielona Góra )\n12:30 Lunch break\n    \n14:00 poster \n    New Insights Into Adaptive Enrichment Designs\n\nRosamarie Frieri (Università di Bologna)Design Problems In Statistical Ecology\n\nLinda Haines (University of Cape Town)Adopting Tolerance Region For The Calibration Problem\n\nCamelia Trandafir (Universidad Pública de Navarra)TBA\nCaterina May (Università del Piemonte Orientale)Optimal Designs For Nonlinear State-Space Models With Applications In Chemical Manufacturing\n\nDasha Semochkina (University of Southampton)Sequential Optimal Planning Of Response Surface Experiments\n\nOlga Egorova (King's College London)How To Determine The Minimal Sample Size In Balanced 3-Way Anova Models Where No Exact \\(F\\)-Test Exists\n\nBernhard Spangl (Universität für Bodenkultur Wien)On The Polytope Of Optimal Approximate Designs\n\nLenka Filova (Comenius University Bratislava)TBA\nVasiliki Koutra (King's College London)A Randomized Exchange Algorithm For Problems With General Atomic Information Matrices\n\nPál Somogyi (Comenius University Bratislava)Optimal Allocation Of Time Points For Consensus Emergence Models\n\nYvette Baurne (Lunds Univeritet)Sequential Bayesian Design Using A Laplace-Parameterised Policy\n\nEmma Rowlinson (University of Manchester)Hierarchical Experiments And Likelihood Approximations\n\nTheodora Nearchou (University of Southampton)TBA\nZiyan Wang (University of Southampton)15:30 Coffee break\n    \n14:00 Posters (Chair: NA)\n    \n16:00 Kernel methods (Chair: TBA)\n    Energy-Based Sampling For PSD-Matrix Approximation\n\nMatthew Hutchings (University of Cardiff)Prediction In Regression Models With Continuous Observations\n\nAndrey Pepelyshev (University of Cardiff)OLSE And BLUE For The Location Model And Energy Minimization\n\nAnatoly Zhigljavsky (University of Cardiff)Kernel Relaxation For Space-Filling Design}\n\nLuc Pronzato (Université Côte d'Azur)\n\n\nWednesday\n\n\n\n\nWednesday\n    12 July 2023\n    09:00 Othogonal minimally aliased response surface and order-of-addition designs (Chair: Peter Goos, KU Leuven)\n    Constructing Large OMARS Designs By Concatenating Definitive Screening Designs\n\nAlan Vazquez (University of Arkansas)TBA\nJose Nunez Ares (KU Leuven)Order-Of-Addition Orthogonal Arrays To Study The Effect Of Treatment Ordering\n\nEric Schoen (KU Leuven)Symmetric Order-Of-Addition Experiments\n\nNicholas Rios (George Mason University)\n10:30 Coffee break\n    \n11:00 Active learning (Chair: Jesús López-Fidalgo, Universidad de Navarra)\n    Implementation Strategies For Model-Robust Designs And Active Learning\n\nXiaojian Xu (Brock University)Accounting For Outliers In Optimal Subsampling Methods\n\nLaura Deldossi (Università Cattolica del Sacro Cuore)Deriving Nearly Optimal Subdata\n\nMin Yang (University of Illinois Chicago)A Covariate Distribution-Based Optimality Criterion For Subdata Selection\n\nÁlvaro Cía Mina (Universidad de Navarra)\n12:30 Lunch break (packed lunch)\n    \n13:30 Excursion to Winchester\n    \n\n\n\nThursday\n\n\n\n\nThursday\n    13 July 2023\n    09:00 Optimal subsampling (Chair: TBA)\n    Treess: A Model-Free Tree-Based Subdata Selection Method For Prediction\n\nJohn Stufken (George Mason University)Scale-Invariant Optimal Sampling And Variable Selection With Rare-Events Data\n\nHaiYing Wang (University of Connecticut)Subsampling In Large Graphs\n\nPing Ma (University of Georgia)Efficient Subsampling For Exponential Family Models\n\nSubhadra Dasgupta (Ruhr-Universität Bochum)\n10:30 Coffee break\n    \n11:00 Design for machine learning and prediction (Chair: TBA)\n    Design Of Experiments And Machine Learning With Application To Industrial Experiments\n\nRoberto Fontana (Politecnico di Torino)Optimal Designs For Prediction In Random Coefficient Regression With One Observation Per Individual\n\nMaryna Prus (Linköping University and Universität Hohenheim)TBA\nHugo Maruri-AguiliarTBA\nAdetola Adediran (University of Southampton)\n12:30 Lunch break\n    \n14:00 Clinical Trials II (Chair: TBA)\n    Longitudinal Model For A Dose-Finding Study For A Rare Disease Treatment\n\nSergei Leonov (CSL Behring)Optimal Relevant Subset Designs In Nonlinear Models\n\nAdam Lane (Cincinnati Children's Hospital Medical Center)Group Sequential Tests - Beyond Exponential Family\n\nSergey Tarima (Medical College of Wisconsin)How Can We Learn About The Biomarker-Negative Subgroup In A Biomarker-Guided Trial? \n\nAnastasia Ivanova (Univerity of North Carolina at Chapel Hill)\n15:30 Coffee break\n    \n16:00 Industrial panel (Chair: Tobias Mielke, Janssen)\n    \nKatrin Roth (Bayer AG)\nAlex Sverdlov (Novartis)\nAndrew Hooker (Uppsala Univeritet)\nRalf-Dieter Hilgers (RWTH Aachen University)\nAlun Bedding (Roche)\nSergei Leonov (CSL Behring)\nOlivier Collignon (GlaxoSmithKline)\n19:30 workshop reception and dinner\n    \n\n\n\nFriday\n\n\n\n\nFriday\n    14 July 2023\n    09:00 TBA (Chair: TBA)\n    Distance In Big Dimensions\n\nJon Gillard (University of Cardiff)Sampling And Low-Rank Approximation \n\nBertrand Gauthier (University of Cardiff)TBA\nBen Parker (Brunel University London)TBA\nNoha Youssef (The American University in Cairo)\n10:30 Coffee break\n    \n11:00 Optimal designs for model discrimination (Chair: Werner Müller, Johannes Kepler Universität Linz)\n    Discrimination Between Gaussian Process Models: Active Learning And Static Constructions\n\nMarkus Hainy (Johannes Kepler Universität Linz)Connection Between Likelihood Tests And Discrimination Designs\n\nChiara Tommasi (Università degli Studi di Milano)Computing \\(T\\)-Optimal Designs Via Nested Semi-Infinite Programming And Two-Fold Adaptive Discretization\n\nDavid Mogalle (Fraunhofer-Gesellschaft)TBA\nJuan Manuel Rodriguez-Diaz\n12:30 Lunch break\n    \n14:00 Workshop closes\n    \n\n\n\nAbstracts\n\n\n\n\nSpeaker\n      Affiliation\n      Title\n      Abstract\n    Maryna Prus\n\nLinköping University and Universität Hohenheim\n\nOptimal Designs For Prediction In Random Coefficient Regression With One Observation Per Individual\n\nThe subject of this work is random coefficient regression models with only one observation per observational unit (individual). An analytical solution in form of optimality conditions is proposed for optimal designs for the prediction of individual random effect for a group of selected individuals. The behaviour of optimal designs is illustrated by the example of linear regression models.\nRoberto Fontana\n\nPolitecnico di Torino\n\nDesign Of Experiments And Machine Learning With Application To Industrial Experiments\n\nIn the context of product innovation, there is an emerging trend to use Machine Learning (ML) models with the support of Design Of Experiments (DOE). The paper aims firstly to review the most suitable designs and ML models to use jointly in an Active Learning (AL) approach. It then reviews ALPERC, a novel AL approach, and proves the validity of this method through a case study on amorphous metallic alloys, where this algorithm is used in combination with a Random Forest model.\nSergei Leonov\n\nCSL Behring\n\nLongitudinal Model For A Dose-Finding Study For A Rare Disease Treatment\n\nDose-finding studies in rare diseases are faced with unique challenges including low\npatient numbers, limited understanding of the dose-exposure-response relationship,\nvariability around the endpoints. In addition, patient exposure to placebo is often not\nfeasible. To describe the disease progression for different dose groups, we introduce\na longitudinal model for the change from baseline for a clinical endpoint. We build\na nonlinear mixed effects model using the techniques which have become popular\nover the past two decades in the design and analysis of population\npharmacokinetic/pharmacodynamic studies. To evaluate operating characteristics of the\nproposed design, we derive the Fisher information matrix and validate analytical results via\nsimulations. Alternative considerations, such as trend analysis, are discussed as well.\nAmalan Mahendran\n\nQueensland University of Technology\n\nModel Robust Subsampling Approach For Generalised Linear Models In Big Data Settings\n\nSubsampling is a computationally efficient and scalable method to support timely insights and informed decision making in big data settings.\nAn integral component of subsampling is determining what subsample should be extracted from the big data for analysis.\nRecent subsampling approaches propose determining subsampling probabilities for each data point based on optimality criteria from experimental design but we suggest this is of limited use in practice as these probabilities rely on an assumed model for the big data.\nTo overcome this limitation, we propose a model robust approach where a set of models is considered, and the subsampling probabilities are evaluated based on the weighted average of probabilities that would be obtained if each model was considered singularly.\nTheoretical support for such an approach is provided and the results from considering a simulation study and two real-world applications show that our model robust approach outperforms current subsampling practices.\nHugo Maruri Aguilar\n\nQueen Mary University of London\n\nSparse Polynomial Prediction\n\nIn numerical analysis, sparse grids are point configurations that are used in stochastic finite element approximation, numerical integration and interpolation. This paper is concerned with the construction of polynomial interpolator models in sparse grids. Our proposal stems from the fact that a sparse grid is an echelon design with a hierarchical structure that identifies a single model. We then formulate the model and show that it can be written using inclusion-exclusion formulæ. At this point, we deploy efficient methodologies from the algebraic literature that can simplify considerably the computations. The methodology uses Betti numbers to reduce the number of terms in the inclusionexclusion while achieving the same result as with exhaustive formulæ.\nSergey Tarima\n\nMedical College of Wisconsin\n\nGroup Sequential Tests - Beyond Exponential Family\n\nWe consider group sequential tests powered for multiple ordered alternative hypotheses with a predetermined \\(\\alpha\\)-spending function. Theorem 1 shows that if a fixed-sample size likelihood ratio test is monotone with respect to a one-dimensional test statistic, then a group sequential test comprised of interim cumulative likelihood ratio tests continue to be monotone. This group sequential test is most powerful, at a given ALPHA-spending function and predetermined interim sample sizes. We work directly with heavy tailed data which provides an enhanced opportunity for the application of group sequential designs in studies of computer network trafficking, high-frequency trading, risk management and insurance, where the use of  heavy tailed distributions is common. This theorem extends our previous results (Metrika, 2022) from the exponential family to non-exponential distributions with monotone likelihood ratio.\nWhen the likelihood ratio is not monotone for finite sample sizes, locally most powerful tests can be constructed if a test is locally most powerful for a fixed sample size against a local alternative. A two-stage Cauchy example is used to show how to build such tests using either a likelihood ratio or an MLE.\nFisher information decomposes into the design and sampling components. The design component is lost to the likelihood. We describe how this affects Cramer-Rao lower bound for post-testing estimators.\nIn summary, if a one parameter distribution for the data is either known or assumed, MLE-based group sequential tests allow for separately powered  multiple ordered alternatives that are most powerful for this set of hypotheses in either finite or in local asymptotic settings. Such constructions can be used to mitigate the over- and under-powering associated with the alternative hypotheses that may be  mis-specified in the design.\nMartin Radloff\n\nUniversität Magdeburg\n\n\\(D\\)-Optimal And Nearly \\(D\\)-Optimal Exact Designs For Binary Response On The Ball\n\nIn this talk the results of Radloff and Schwabe (Stat Papers 60:165–177, 2019) will be extended for a special class of symmetrical intensity functions. This includes binary response models with logit and probit link. To evaluate the position and the weights of the two non-degenerated orbits on the \\(k\\)-dimensional ball usually a system of three equations has to be solved. The symmetry allows to reduce this system to a single equation. As a further result, the number of support points can be reduced to the minimal number. These minimally supported designs are highly efficient.\nJuan Rodríguez-Díaz\n\nUniversidad de Salamanca\n\nConstruction Of Maxi-Min Efficiency Designs\n\nMaxi-min efficiency criteria are commonly applied to solve the issue of parameter dependence in non-linear problems. They can also be used to take into consideration several tasks expressed by different component-wise criteria. Maxi-min efficiency criteria are, however, difficult to manage because of their lack of differentiability. As a consequence, maxi-min efficiency designs are frequently built through heuristic and ad hoc algorithms, without the possibility of checking for their optimality. The main contribution of this study is to prove that the maxi-min efficiency optimality is equivalent to a Bayesian criterion, which is differentiable. In addition, we provide an analytic method to find the prior probability associated with a maxi-min efficient design, making feasible the application of the equivalence theorem. Two illustrative examples show how the proposed theory works.\nTorsten Reuter\n\nUniversität Magdeburg\n\nOptimal Subsampling Design For Polynomial Regression In One Covariate\n\nImprovements in technology lead to increasing availability of large data sets which makes the need for data reduction and informative subsamples ever more important. We construct \\(D\\)-optimal subsampling designs for polynomial regression in one covariate for invariant distributions of the covariate. We study quadratic regression more closely for specific distributions. In particular we make statements on the shape of the resulting optimal subsampling designs and the effect of the subsample size on the design. We propose a generalization of the IBOSS method to quadratic regression which does not require prior knowledge of the distribution of the covariate and which performs remarkably well compared to the optimal subsampling design.\nArno Strouwen\n\n\nAdaptive And Robust Experimental Design For Linear Dynamic Models Using The Kalman Filter\n\nCurrent experimental design techniques for dynamical systems often only incorporate measurement noise, while dynamical systems also involve process noise. To construct experimental designs we need to quantify their information content. The Fisher information matrix is a popular tool to do so. Calculating the Fisher information matrix for linear dynamical systems with both process and measurement noise involves estimating the uncertain dynamical states using a Kalman filter. The Fisher information matrix, however, depends on the true but unknown model parameters. In this presentation we combine two methods to solve this issue and develop a robust experimental design methodology. First, Bayesian experimental design averages the Fisher information matrix over a prior distribution of possible model parameter values. Second, adaptive experimental design allows for this information to be updated as measurements are being gathered. This updated information is then used to adapt the remainder of the design.\nRosamarie Frieri\n\nUniversità di Bologna\n\nOptimal Design For Inference On The Threshold Of A Biomarker\n\nEnrichment designs with a continuous biomarker require the estimation of a threshold to determine the subpopulation benefitting from the treatment.   This paper provides the optimal allocation for inference in a two-stage enrichment design for treatment comparisons when a continuous biomarker is suspected to affect patient response. Several design criteria, associated with different trial objectives, are optimized under balanced or Neyman allocation and under equality of the first two empirical biomarker’s moments. Moreover, we propose a new covariate-adaptive randomization procedure that converges to the optimum with the fastest available rate. Theoretical and simulation results show that this strategy improves the efficiency of a two-stage enrichment clinical trial, especially with smaller sample sizes and under heterogeneous responses.\nRosamarie Frieri\n\nUniversità di Bologna\n\nNew Insights Into Adaptive Enrichment Designs\n\nThe transition towards personalized medicine is happening and the new experimental framework is raising several challenges, from a clinical, ethical, logistical, regulatory, and statistical perspective. To face these challenges, innovative study designs with increasing complexity have been proposed. In particular, adaptive enrichment designs are becoming more attractive for their flexibility. However, these procedures rely on an increasing number of parameters that are unknown at the planning stage of the clinical trial, so the study design requires particular care. This review is dedicated to adaptive enrichment studies with a focus on design aspects. While many papers deal with methods for the analysis, the sample size determination and the optimal allocation problem have been overlooked. We discuss the multiple aspects involved in adaptive enrichment designs that contribute to their advantages and disadvantages. The decision-making process of whether or not it is worth enriching should be driven by clinical and ethical considerations as well as scientific and statistical concerns.\nNicholas Rios\n\nGeorge Mason University\n\nSymmetric Order-Of-Addition Experiments\n\nIn an Order-of-Addition (OofA) experiment, the order in which \\(m\\) components are added to a system influences a response. The goal of an OofA experiment is to find an optimal permutation; i.e., one that maximizes or minimizes the response. Existing models for Order-of-Addition experiments, such as the Pairwise-Ordering (PWO) model, assume that ordering effects are asymmetric. This assumption is unsuitable for many problems in networking, such as the famous Travelling Salesman Problem, where the cost of traversing a network is the same moving forwards as it is backwards. In this scenario, a permutation can be viewed as a Hamiltonian path on an undirected graph. A model is proposed that uses the edges of Hamiltonian paths to represent the effect of a permutation. Optimal designs for this model are derived that only require a fraction of all possible Hamiltonian paths to be examined. The model is shown to be effective at finding optimal paths in a drone delivery problem.\nFrank Röttger\n\nUniversité de Genève\n\nOptimal Designs For Discrete Choice Models Via Graph Laplacians\n\nIn this work, we connect design theory for discrete    choice experiments with Laplacian matrices of connected graphs.\nWe rewrite the \\(D\\)-optimality criterion in terms of Laplacians via Kirchhoff’s matrix tree theorem, and show that its dual has a simple description via the Cayley–Menger determinant of the Farris transform of the Laplacian matrix.\nThis results in a drastic reduction of complexity and allows us to implement a gradient descent     algorithm to find locally \\(D\\)-optimal designs.\nFor the subclass of Bradley–Terry paired comparison models, we find a direct link to maximum likelihood estimation for Laplacian-constrained Gaussian graphical models.\nThis implies that every locally \\(D\\)-optimal design is a rational function in the parameter when the design is supported on a chordal graph. Finally, we study the performance of our algorithm and demonstrate its application on real data.\nRosemary Bailey\n\nUniversity of St Andrews\n\nValid Restricted Randomization For Small Experiments\n\nIf there is no inherent blocking factor in a small experiment, it may be decided not to use blocks, in order to have more degrees of freedom for the residual and hence more power for detecting treatment differences. However, in that case, complete randomization may produce a long run of plots with the same treatment.  How should that be avoided? One common suggestion is simply to  discard the undesirable layout and randomize again. This introduces bias, as it makes comparisons between neighbouring plots more likely to contribute to the estimators of treatment differences.When there is a single error term in the analysis of variance, a method of randomization  is called strongly valid if the expected mean square for any subset of treatment comparisons is equal to the expected mean square for error if there are no differences between treatments.  Here all these mean squares are averaged over all possible outcomes of the randomization. See Gundy & Healy (1950) and Yates (1948).One way of achieving strongly valid randomization is to choose a permutation at random from a doubly-transitive permutation group.  Applying a random permutation from such a group to a carefully chosen initial layout has the potential to avoid some bad patterns.  See Bailey (1983) and Gundy & Healy (1950). In the context of clinical trials with only two treatments and sequential recruitment of patients, there is a  second method using Hadamard matrices. See Bailey & Nelson (2003).  Using this avoids the risk of large treatment imbalance if the trial is terminated early, as well as of long runs of a single treatment.Yates (1948) proposed the term restricted randomization for any valid method that does not include the all layouts.  Unfortunately, Youden (1972) introduced the term constrained randomization  for the same thing.  His method implicitly uses resolved balanced incomplete-designs. In my talk I  shall describe recent joint work with Josh Paik using this method to produce a catalogue of tables which give a method of valid randomization for small experiments with a single line of experimental units.\nBailey, R. A.: Restricted randomization. Biometrika 70  (1983), 183–198.\nBailey, R. A. and  Nelson, P. R.: Hadamard randomization: a valid restriction of random permuted blocks. Biometrical Journal 45  (2003), 554–560.\nGrundy, P. M. and Healy, M. J. R.: Restricted randomization and quasi-Latin squares. Journal of the Royal Statistical Society, Series B 12  (1950), 286–291.\nYates, F.:  Contribution to the  discussion of the paper “The validity of comparative experiments” by F. J. Anscombe. Journal of the Royal Statistical Society, Series~A 111 (1948), 204–205.\nYouden, W. J.: Randomization and experimentation. Technometrics 14 (1972), 13–22.\nAnthony Atkinson\n\nLondon School of Economics\n\nOptimum Design For Ill-Conditioned Models: \\(K\\)–Optimality And Stable Parameterizations\n\nLeast squares estimation of the parameters in ill-conditioned nonlinear models may lead, via multiple optima and computational inefficiency, to virtually collinear parameter estimates; model building is then problematic.\nA strategy suggested by Gavin Ross is to reparameterize the one-factor model; the  original set of parameters being replaced by another with increased orthogonality. Ross finds such stable parameters as functions of the response at selected values of the factor, the points being chosen in an ad hoc manner for each application.\nIn this paper we propose a systematic strategy for model reparameterization\nbased on a carefully chosen set of points.\nThis is illustrated with the support points of locally \\(K\\)–optimal experimental\ndesigns, to generate a set of symbolic equations that allow the construction\nof a transformation to a set of parameters with better orthogonality\nproperties. Recognizing the difficulties in the generalization of the\ntechnique to complex models, we propose a related alternative approach based\non first-order Taylor approximation of the model. Our approach is tested both\nwith linear and nonlinear models. The Variance Inflation Factor and the\ncondition number as well as the orientation and eccentricity of the parametric\nconfidence region are used for comparisons.\nLinda Haines\n\nUniversity of Cape Town\n\nDesign Problems In Statistical Ecology\n\nThe density of animals  in regions ranging from small areas such as game reserves to vast areas over which animals are threatened with extinction is of key importance in ecology and  conservation. It is not generally possible to conduct a census of animals in a region of interest and recourse is therefore made to observations, such as counts and times of arrival, at chosen study sites or transects across the region. Such surveys are expensive in terms of cost and human effort and design therefore plays a crucial role in the planning process. In this poster, I will highlight design issues relating to the estimation of animal density from observations based on site visits and capture-recapture.\nMore specifically, I will introduce two interesting examples of ecological settings in which subtle design strategies could  and should be formulated.\nAdam Lane\n\nCincinnati Children’s Hospital Medical Center\n\nOptimal Relevant Subset Designs In Nonlinear Models\n\nFisher (1934) argued that certain ancillary statistics form a relevant subset, a subset of the sample space on which inference should be restricted, and showed that conditioning on such ancillary statistics reduces the dimension of the data without a loss of information. The use of ancillary statistics in post-data inference has received significant attention; however, their role in the design of experiments has not been well characterized. Ancillary statistics are unknown prior to data collection and as a result cannot be incorporated into the design a priori. Conversely, in sequential experiments the ancillary statistics based on the data from the preceding observations are known and can be used to determine the design assignment of the current observation. The main results of this work describe the benefits of incorporating ancillary statistics, specifically, the ancillary statistic that constitutes a relevant subset, into adaptive designs.\nAnastasia Ivanova\n\nUniverity of North Carolina at Chapel Hill\n\nHow Can We Learn About The Biomarker-Negative Subgroup In A Biomarker-Guided Trial? \n\nIn a clinical trial with a predefined subgroup, it is assumed that the biomarker positive subgroup has the same or higher treatment effect compared to its complement, the biomarker negative subgroup. In these trials the treatment effect is usually evaluated in the biomarker positive subgroup and in the whole population. Statistical testing of the treatment effect in the biomarker negative subgroup is usually not done since it requires a larger sample size. As a result, the new intervention can be shown effective in the overall population even though it is only effective in the biomarker positive group. What can we do to improve decision making in such trials?\nCamelia Trandafir\n\nUniversidad Pública de Navarra\n\nAdopting Tolerance Region For The Calibration Problem\n\nIt is known that the simple Linear Calibration problem is based on the evaluation of the “future calibrating value”, which actually means to obtain an optimal ratio estimate, under different optimality criteria. Different approaches have been proposed to face the problem. But for these “future” observations is requested a high proportion of them, to be within a certain interval, for a given probability level. This is exactly the definition of the Tolerance Region (TL). There is a different line of thought to face the TL and under the different approach, different solutions are offered for the Calibration problem, which are presented to this paper.\nKatrin Roth\n\nBayer AG\n\nChallenges And Pitfalls In Applying Optimal Design Theory In Clinical Dose Finding Studies\n\nIn designing clinical dose finding studies, challenges arise not only through operational limitations like pre-defined tablet strengths, but also as different objectives need to be addressed and the planning assumptions are highly variable. For example, in trials using MCP-Mod one objective is to test for a dose-response relationship using a contrast test, while another objective is to estimate the dose-response relationship. Maximizing the power of the contrast test and maximizing the precision in estimating the dose-response curve generally require different optimality criteria and thus result in different optimal designs. Also, an efficient design across several candidate models needs to be found as the shape of the dose-response curve might be unknown. Even if a design that is efficient regarding both the contrast test and the estimation of several candidate models is chosen, deviations from the planning assumptions or just unfortunate data constellations can lead to unfavorable results. These may include convergence issues in estimating specific models, or large confidence intervals in case the chosen design was inefficient for the resulting estimated parameters. We will present an example of a real dose-finding study where we encountered such challenges and will provide some initial ideas how to improve the design in such situations.\nCarlos de la Calle Arroyo\n\nUniversidad de Navarra\n\nA Methodology To Augment Designs\n\nOptimal experimental designs usually have too few points and often very extremal, with points lying on the boundary of the design space. It is common that the number of different points is the same as the number of parameters to be estimated for models with a single explanatory variable. This does not allow for proper model adequacy testing. Due to this and other reasons, such as constraints, particular domain practice or need for robust estimation. Optimal designs; therefore, are often used as a reference to measure how efficient are the designs used in practice. In this work, as an alternative, a methodology for design augmentation is proposed. Based on the equivalence theorem, the procedure allows to control the efficiency when adding points to a given design. The experimenter can then, starting from the optimal design, or a regulated experimental plan, add points controlling the efficiency in order to enhance the initial design to its liking. The full procedure is presented for \\(D\\)-optimality, while an analogous solution for \\(D_S\\)- and \\(L\\)-optimality is tentatively provided. Its software implementation is available within the R package optedr.\nDasha Semochkina\n\nUniversity of Southampton\n\nOptimal Designs For Nonlinear State-Space Models With Applications In Chemical Manufacturing\n\nOne of the first stages in chemical manufacturing is to create and calibrate a realistic model of the process of interest. Many chemical reactions can be expressed as nonlinear state-space models which are also widely used in other areas, including statistics, econometrics, information engineering and signal processing. State-space models depend on parameters to be calibrated and some control parameters.  We address the problem of systematic optimal experimental design for the control parameters for this class of models. We construct locally D-optimal designs by incorporating the calculation of the determinant of the Fisher Information Matrix. This allows us to identify a set of control parameters such that if experiments are run under those conditions, the remaining parameters could be estimated with the highest possible precision.\nOlga Egorova\n\nKing’s College London\n\nSequential Optimal Planning Of Response Surface Experiments\n\nThis work explores a stage-by-stage planning of factorial experiments with multiple objectives, incorporating both the quality of the inference from the fitted model and the protection against potential model misspecification. We adapt stratum-by-stratum design search strategy and the Pareto front approach to obtain a set of optimal designs to assist the practitioners in making informed decisions.\nBernhard Spangl\n\nUniversität für Bodenkultur Wien\n\nHow To Determine The Minimal Sample Size In Balanced 3-Way Anova Models Where No Exact \\(F\\)-Test Exists\n\nWe consider balanced three-way ANOVA models to test the hypothesis that the fixed factor \\(A\\) has no effect. The other factors are fixed or random. For most of these models (including all balanced 1-way and 2-way ANOVA models) an exact \\(F\\)-test exists. Details on the determination of the minimal sample size and on an in-depth structural result can be found in Spangl, et al.(2023).For the two models\n\\begin{equation}  \\label{eq:approx}\nA \\times \\boldsymbol{B} \\times \\boldsymbol{C}\n\\qquad\\text{and}\\qquad\n(A \\succ \\boldsymbol{B}) \\times \\boldsymbol{C}%,\n\\end{equation}\n(bold letters indicate random factors), however, an exact \\(F\\)-test\ndoes not exist. Approximate \\(F\\)-tests can be obtained by Satterthwaite’s\napproximation. The approximate \\(F\\)-test involves mean squares to be\nsimulated. To approximate the power of the test, we simulate data such\nthat the null hypothesis is false and we compute the rate of\nrejections. The rate then approximates the power of the test.In this talk we aim to determine the minimal sample size of the two\nmodels mentioned above, given a prespecified power, and we\ngive a heuristic that the number of replicates \\(n\\) should be\nkept small \\((n=2)\\). This suggestion is backed by all simulation results. This\nleaves the numbers of levels of the random factors \\(B\\) and \\(C\\) to determine\nthe sample size.\ndetermine the active and inactive variance components for both\nANOVA models by using a surrogate fractional factorial model with\nvariance components as factors.\ndetermine the worst combination of active variance components\nfor both models by using a surrogate response surface model based on a\nBox-Behnken design. The special structure of the Box-Behnken design\nensures that the used models have similar total variance.Additionally we propose three practical methods that help reducing the\nnumber of simulations required to determine the minimal sample size.The first suggested method searches for models with minimal sample\nsize in the proximity of a preselected ray on the grid spanned by\nthe numbers of levels of the random factors \\(B\\) and \\(C\\).The second method optimizes the sample size subject to a\nprespecified power by using response surface methods.The third method reconstructs the power surface by fitting a\nparametric model to simulated empirical power values. This\nreconstruction is then used to determine the model with minimal sample\nsize given a prespecified power.We compare the proposed methods, present some examples and, finally,\nwe give recommendations about which method to choose.Spangl, B., Kaiblinger, N., Ruckdeschel, P. & Rasch, D. (2023). Minimal sample size in balanced ANOVA models of crossed, nested, and mixed classifications. Communications in Statistics – Theory and Methods, 52(6), 1728–1743.\nLenka Filova\n\nComenius University Bratislava\n\nOn The Polytope Of Optimal Approximate Designs\n\nWe study the problem of non-uniqueness of optimal approximate designs (OADs) in regression experiments with uncorrelated observations. We show that the set of all OADs can be typically characterized by a polytope P whose extreme points form a finite set A. We call the elements of the set A ‘atomic optimal designs’ and show that, for models with non-unique OADs, the list of all atomic optimal designs enables us to choose the OAD that best corresponds to the needs of the experimenter. For example, it can be used to construct OADs with minimal support, or OADs optimizing a secondary criterion. We illustrate the results on the k-way second-degree model without interactions.\nPál Somogyi\n\nComenius University Bratislava\n\nA Randomized Exchange Algorithm For Problems With General Atomic Information Matrices\n\nThe optimal design algorithm REX (see Harman et al, 2020) is simple and efficient, yet it is limited to the basic optimality criteria and rank-one elementary information matrices. This contribution proposes a generalization of the REX algorithm, expanding its capabilities to compute optimal approximate designs with respect to all Kiefer’s optimality criteria and elementary information matrices of any rank. In addition to the generalization of the REX algorithm, we also present some applications of optimal design problems with general-rank elementary information matrices, such as multivariate response models and optimal augmentation of designs. Numerical results confirm the stability and rapid convergence of the proposed algorithm.\nHarman, R., Filová, L.,Richtárik, P. (2020): A Randomized Exchange Algorithm for Computing Optimal Approximate Designs of Experiments, Journal of the American Statistical Association, Vol. 115, No. 529, 348-361.\nYvette Baurne\n\nLunds Univeritet\n\nOptimal Allocation Of Time Points For Consensus Emergence Models\n\nWe study the optimal allocation of time points for consensus emergence models. The models are of interest within the social sciences and the study of organisations and groups, to study how within-group variance changes over time. More specifically we consider two consensus emergence models; (i) the heterogeneous consensus emergence model and (ii) the homogeneous consensus emergence model. The models are mixed models with a three-level nested structure, where the levels correspond to groups, individuals, and repeated measurements. Model (i) is a linear mixed model with an exponential decay of the noise variance. Model (ii) is a type of growth curve model, with exponential decay at the second level. The design problem is non-trivial because of the exponential functions and interest in variance components, making the problem similar to the problem of nonlinear models - the optimal design will depend on the parameter values of interest. We formulate the problem for complete data and extend it to incorporate intermittent missing data. As a first step we assume non-informative missing data.\nEmma Rowlinson\n\nUniversity of Manchester\n\nSequential Bayesian Design Using A Laplace-Parameterised Policy\n\nPolicy-based approaches pose a promising future direction for sequential Bayesian design. These approaches involve use of a design policy, which maps from the current state of knowledge to the next proposed design. The design policy must first be trained via tuning of its parameters to maximise the expected utility of the whole sequential experiment, referred to as total expected information gain (total EIG). This can potentially enable better performance compared to traditional “myopic” or one-step look ahead approaches.\nA key question is how to represent the current state of knowledge, or equivalently, parameterise the design policy. Previous works have considered the use of neural network policies with relatively little structure. Our work explores the use of a Laplace approximation to the posterior to parameterise the current state of knowledge. This approach allows us to incorporate more refined structure into the policy, potentially offering a more lucid interpretability and efficient policy training. Our research also explores whether this Laplace-parameterised policy achieves higher total EIG in sequential Bayesian design problems.\nMotivated by recent results demonstrating Laplace-based importance sampling approximations give highly accurate approximations to the EIG utility in Bayesian design, we use Approximate Laplace Importance Sampling (ALIS) to estimate the intractable total EIG utility.\nTo implement our methodology, we have utilised PyTorch and its automatic differentiation package, torch.autograd. We apply the proposed approach to a location finding example, where we endeavour to determine the true locations of sources, given measurements of a signal. The poster presented will summarise progress on this work so far.\nTheodora Nearchou\n\nUniversity of Southampton\n\nHierarchical Experiments And Likelihood Approximations\n\nThis work aims to approximate likelihood inference for models with intractable likelihood by combining various approximations to the likelihood, including cheap crude approximations and more computationally expensive accurate approximations to obtain statistically valid and effi- cient inference. To do this, we use techniques from the field of computer experiments, where Gaussian process (GP) models can be used as a surrogate for the computer model output. We apply multi-level GP model-based approximations where two levels of computer model are avail- able, corresponding to two levels of likelihood approximations. We illustrate that the likelihood approximation can be predicted using few observations from the complex approximation and more from the simple approximation through a generalised linear mixed model example. More- over, we present a methodology for choosing the design for our two-level GP approximation, with the aim of finding the point that maximises the likelihood using expected gain in utility.\nNancy Flournoy\n\nUniversity of Missouri\n\nPosterior Alternatives With Informative Early Stopping\n\nProminent Bayesian scholars (e.g., Berry and Ho, 1988) argue that Bayesian philosophy permits multiple interim decisions to stop or continue a trial without adjustment or penalty. In contrast,  Frequentist practice is to adjust stopping boundaries to control Type~1 error, recognizing that without adjustment it converges to one as the number of interim tests increases.\nIt is standard Bayesian practice that when more data become available, the posterior distribution is updated with new information and the posterior becomes the prior for the next posterior analysis.  Yet in experiments with informative interim stopping decisions, standard Bayesian practice is not to condition the sampling density on interim decisions that are made.  The consequence is that information in the decision is lost and the likelihood is invariant to the decision.    Because it is also standard Bayesian philosophy that an analysis be performed on the experiment that was actually run, and not on experiments that might have been run,  we propose incorporating information about the interim decision into the post-interim-decision posterior.\nWe explore the  consequences  of conditioning the sampling density on the interim decision for subsequent posterior analyses in the context of a two-stage design with an early stopping option. Our discussion avoids relying on distributions of  Frequentist statistics which are fixed under the Bayesian paradigm.\nBill Rosenberger\n\nGeorge Mason University\n\nDesign And Inference For Enrichment Trials With A Continuous Biomarker\n\nWe describe the philosophical approaches to two-stage enrichment designs, in which a benefitting subpopulation is targeted in a second stage, after a first stage identifies the threshold of a predictive, continuous biomarker.  The design issue we address is sample size estimation for the first and second stages, and the consequences of poorly estimating the threshold.  These design issues are established based on an approach where the two stages are conducted and analyzed separately, and stage two is considered a confirmatory trial.  Another approach is to combine the data from the two stages, and we demonstrate how to do that by testing two hypotheses simultaneously with test statistics that (we show) have an asymptotic normal distribution.  While a bivariate normal model is used to give insights into the predictive nature of the biomarker, and to visualize some closed-form solutions, in principle other models can certainly be used (but perhaps yield fewer insights).  As in many ongoing, long-term research projects, our work probably raises more questions than it answers!\nFrank Miller\n\nLinköping and Stockholm Universities\n\nAbout \\(C\\)- And \\(D\\)-Optimal Dose-Finding Designs For Bivariate Outcomes\n\nIn many cases, clinical dose-finding trials consider both efficacy and safety. In order to optimise the trial design, we analyse bivariate models for these two outcomes. We focus on models of Emax-type for the outcomes with different number of parameters. We discuss results for the number of design points for the locally \\(c\\)- and \\(D\\)-optimal designs. The considered family of models offers the opportunity to use symmetry properties and transformations of the design space. Using these methods, we can derive algebraic results for the optimal designs. We illustrate how the optimal design and its number of design points depend on model parameters. Since the locally optimal designs depend on unknown model parameters, we handle this issue using sequential designs.\nMarkus Hainy\n\nJohannes Kepler Universität Linz\n\nDiscrimination Between Gaussian Process Models: Active Learning And Static Constructions\n\nWe consider the design and analysis of experiments to discriminate between two Gaussian process models with different covariance kernels, such as those widely used in computer experiments, kriging, sensor location and machine learning. Two frameworks are considered. First, we study sequential constructions, where successive design points are selected, either as additional points to an existing design or from the beginning of observation. We investigate criteria such as the symmetrised Kullback-Leibler divergence between the two models when some observations have already been collected and the mean squared error of both models when no prior observations are available. Furthermore, we consider static criteria, such as the familiar log-likelihood ratios and the Fréchet distance between the covariance functions of the two models. Other distance-based criteria, simpler to compute than previous ones, are introduced. These new criteria can be easily generalised in terms of design measures, so we can use the framework of approximate design and provide a necessary condition for the optimality of a design measure.\nChiara Tommasi\n\nUniversità degli Studi di Milano\n\nConnection Between Likelihood Tests And Discrimination Designs\n\nThe goal of this study is to show the connection between hypothesis testing and optimal designing to discriminate between homoscedastic and heteroscedastic regression models. More specifically, we consider the following heteroscedastic regression model for the response \\(Y\\):\n$$\ny_i=\\eta(\\boldsymbol{x}_i;\\boldsymbol{\\beta})+\\varepsilon_i,; \\varepsilon_i\\sim N(0;\\sigma^2 h(\\boldsymbol{x}_i;\\boldsymbol{\\gamma})),\\quad i=1,\\ldots,n\n$$\nwhere \\(\\eta(\\boldsymbol{x}_i;\\boldsymbol{\\beta})\\) is the mean response, \\(\\boldsymbol{\\beta}\\) is a vector of regression coefficients and \\(\\sigma^2 h(\\boldsymbol{x}_i;\\boldsymbol{\\gamma})\\)  is the error\nvariance depending on an unknown constant \\(\\sigma^2\\) and a continuous positive function \\(h(\\cdot; \\cdot)\\),\ncompletely known except for a parameter vector  \\(\\boldsymbol{\\gamma}\\in {\\rm I\\!R}^s\\). Let \\(\\boldsymbol{\\gamma}_0\\) such that \\(h(\\boldsymbol{x}; \\boldsymbol{\\gamma}_0) = 1\\) (homoscedastic case).\nTo test \\(H_0 :\\, \\boldsymbol{\\gamma}=\\boldsymbol{\\gamma}_0\\) against a local alternative\n\\(H_1 :\\, \\boldsymbol{\\gamma} = \\boldsymbol{\\gamma}_0 + \\boldsymbol{\\lambda}/\\sqrt{n}\\) (with \\(\\boldsymbol{\\lambda}\\neq \\boldsymbol{0}\\)), a likelihood-based test  (such as log-likelihood ratio, score or Wald statistics)  is usually applied.\nWe aim at designing an experiment with the goal of maximizing (in some\nsense) the asymptotic power of a likelihood-based test. Few papers\nare related to hypothesis testing (see for instance,  Dette and Titoff (2009) and the references therein) and herein, we justify the use of the \\(D_s\\)-criterion and the KL-optimality\n(López-Fidalgo, Tommasi, Trandafir, 2007) to design an experiment with the inferential\ngoal of checking for heteroscedasticity. Both \\(D_s\\)- and KL-criteria are proved to be related to the\nnoncentrality parameter of the asymptotic chi-squared distribution of a likelihood test.\nDette, H. and Titoff, S.(2009). Optimal discrimination designs. The Annals of Statistics, 37(4), 2056-2082.\nLópez-Fidalgo, J. and Tommasi, C. and Trandafir, P. C.(2007). An optimal experimental design criterion for discriminating between non-normal models. Journal of the Royal Statistical Society B, 69(2), 231–242.\nDavid Mogalle\n\nFraunhofer-Gesellschaft\n\nComputing \\(T\\)-Optimal Designs Via Nested Semi-Infinite Programming And Two-Fold Adaptive Discretization\n\nThe \\(T\\)-criterion for model discrimination represents a bi-level optimization problem which can be transferred into a semi-infinite one. However, its solution is very unstable or time consuming for non-linear models and non-convex lower- and upper-level problems. If one considers only a finite number of possible design points, a numerically well tractable linear semi-infinite optimization problem arises. Since this is only an approximation of the original model discrimination problem, we propose an algorithm which alternately and adaptively refines discretizations of the parameter as well as of the design space and, thus, solves a sequence of linear semi-infinite programs. We prove convergence of our method and its subroutine and show on the basis of discrimination tasks from process engineering that our approach is stable and can outperform the known methods.\nJohn Stufken\n\nGeorge Mason University\n\nTreess: A Model-Free Tree-Based Subdata Selection Method For Prediction\n\nWith ever larger datasets, there is a growing need for methods that select just a small portion of the entire dataset (subdata) so that reliable inferences can be obtained by analyzing only the selected subdata. Many of the subdata selection methods that have been proposed in recent years are based on model assumptions for the data. While these methods can work extremely well when the model assumptions hold, they may yield poor results if the assumptions are wrong. In addition, subdata that is good for one task may not be so good for another. In this presentation we introduce and discuss a model-free Tree-based Subdata Selection method (TreeSS) that focuses on selecting subdata that performs well for prediction.\nHaiYing Wang\n\nUniversity of Connecticut\n\nScale-Invariant Optimal Sampling And Variable Selection With Rare-Events Data\n\nSubsampling is a particularly effective approach to solve the computational challenges with massive rare-events data, with the possibility of a significant reduction in computational burden by little sacrifice on asymptotic estimation efficiency. In case of any estimation efficiency loss due to too aggressive subsampling, an optimal subsampling method can help minimize the information loss. However, optimal subsampling has never been investigated in the context of variable selection. Existing optimal subsampling probabilities based on \\(A\\)- and \\(L\\)- optimalities depend on the scale of the covariates and may produce inconsistent results for the same data with different scale transforms. This scale dependence issue may cause more serious problems in variable selection when there are inactive covariates, because the contribution of the inactive covariates may be arbitrarily amplified if an inappropriate scale transform. To resolve this issue and fill the aforementioned gap in the literature, we investigate variable selection for rare-events data. We first prove the oracle properties of full data adaptive lasso estimator with massive rare-events data, which justify the usage of subsampling controls. We then propose a scale invariant optimal subsampling function to minimize the prediction error of the inverse probability weighted (IPW) adaptive lasso. Both the optimal subsampling function and the adaptive lasso require a pilot estimator, and the two procedures are naturally integrated. We also propose an estimator based on maximum sampled conditional likelihood with adaptive lasso penalty to further improve the estimation efficiency. The oracle properties of the proposed estimator are also investigated. Numerical experiments based on simulated and real data are carried out to investigate the performances of proposed methods.\nPing Ma\n\nUniversity of Georgia\n\nSubsampling In Large Graphs\n\nIn the past decades, many large graphs with millions of nodes have been collected/constructed. The high computational cost and significant visualization difficulty hinder the analysis of large graphs. Researchers have developed many graph subsampling approaches to provide a rough sketch that preserves global properties. By selecting representative nodes, these graph subsampling methods can help researchers estimate the graph statistics, e.g., the number of communities, of the large graph from the subsample. However, the available subsampling methods, e.g., degree node sampler and random walk sampler, tend to leave out minority communities because nodes with high degrees are more likely to be sampled.In this talk, I present a novel subsampling method via an analog of Ricci curvature in\nmanifolds, i.e., Ollivier Ricci curvature.\nSubhadra Dasgupta\n\nRuhr-Universität Bochum\n\nEfficient Subsampling For Exponential Family Models\n\nWe propose a novel two-stage subsampling algorithm based on optimal design principles. In the first stage, we use a density-based clustering algorithm to identify an approximating design space for the predictors from an initial subsample and determine an optimal approximate design on this design space.  In the second stage,  we use matrix distances such as the Procrustes, Frobenius, and square-root distance to define the remaining subsample, such that its points are “closest” to the support points of the optimal design. Our approach reflects the specific nature of the information matrix as a  weighted sum of non-negative definite Fisher information matrices evaluated at the design points and is applicable to a large class regression models including models where the Fisher information is of rank larger than \\(1\\).\nXiaojian Xu\n\nBrock University\n\nImplementation Strategies For Model-Robust Designs And Active Learning\n\nWe discuss the implementation strategies for model-robust experimental designs and active learning, which protect against possible model departures within a L2-type of neighbourhood. Accounting for both variance minimization and bias reduction, the minimax efficient designs are necessary to be absolutely continuous. In the present study, we propose two new implementation methods: one is deterministic, a ìcluster designî whereas the other is non-deterministic, a ìrandomized designî. We compare these implementation strategies among other existing ones in the literature.\nLaura Deldossi\n\nUniversità Cattolica del Sacro Cuore\n\nAccounting For Outliers In Optimal Subsampling Methods\n\nNowadays, in many different fields, massive data are available and for several reasons, it might be convenient to analyze just a subset of the data. The application of the \\(D\\)-optimality criterion can be helpful to optimally select a subsample of observations. However, it is well known that \\(D\\)-optimal support points lie on the boundary of the design space and if they go hand in hand with extreme response values, they can have a severe influence on the estimated linear model (leverage points with high influence). To overcome this problem, firstly, we propose a non- informative “exchange” procedure that enables us to select a “nearly” \\(D\\)-optimal subset of observations without high leverage values.\nThen, we provide an informative version of this exchange procedure, where besides high leverage points also the outliers in the responses (that are not necessarily associated to high leverage points) are avoided. This is possible because, unlike other design situations, in subsampling from big datasets the response values may be available.\nFinally, both the non-informative and informative selection procedures are adapted to \\(I\\)-optimality, with the goal of getting accurate predictions.\nMin Yang\n\nUniversity of Illinois Chicago\n\nDeriving Nearly Optimal Subdata\n\nBig data brings unprecedented challenge of analyzing such data due to its extraordinary size. One strategy of analyzing such massive data is data reduction. Instead of analyzing the full dataset, a selected subdata set is analyzed. Various subdata selection methods have been proposed. Those methods are often based on the characterization of an optimal design. There are significant differences between optimal design and subdata selection: (i) an optimal design point may not exist in a given full data and (ii) while a point can be selected multiple times in an optimal design, it can only be selected once in a subdata selection. While the trade-off between computation complexity and statistical efficiency has been studied, little is known how efficient the selected subdata is in terms of statistical efficiency. To answer this question, we need to find an optimal subdata. Deriving an optimal subdata, however, is a N-P hard problem. In this talk, a novel framework to derive a nearly-optimal subdata, under any given statistical model, regardless of optimality criterion or parameters of interest, will be introduced. This framework has three benefits: (i) it shows us the structure of a nearly-optimal subdata for any given full data under various set-ups (model, optimality criterion, parameter of interest); (ii) it measures highly accurate statistical efficiency; and (iii) it provides a tool of deriving a nearly optimal subset in active learning where statistical efficiency is the main concern.\nÁlvaro Cía Mina\n\nUniversidad de Navarra\n\nA Covariate Distribution-Based Optimality Criterion For Subdata Selection\n\nDownsizing the data volume through subsampling is a widely employed technique to efficiently compute estimators in regression models. While existing methods predominantly focus on reducing parameter estimation errors, the primary practical objective of statistical models is often to minimize prediction errors. In this study, we introduce a novel subdata selection method for linear models based on the distribution of covariates. Specifically addressing scenarios with large samples where acquiring labels for the response variable is expensive, our proposed approach is supported by theoretical justifications and aligns with standard linear optimality criteria. Sequential selection is also considered. As anticipated by the theory, our method exhibits a reduction in prediction mean squared error compared to existing methods. Through simulations, we illustrate the performance of our innovative approach and its potential to enhance prediction accuracy in linear models.\nAlan Vazquez\n\nUniversity of Arkansas\n\nConstructing Large OMARS Designs By Concatenating Definitive Screening Designs\n\nOrthogonal minimally aliased response surface (OMARS) designs permit the screening of quantitative factors at three levels using an economical number of runs. In these designs, the main effects are orthogonal to each other and to the quadratic effects and two-factor interactions of the factors, and these second-order effects are never fully aliased. Complete catalogs of OMARS designs with up to seven factors have been obtained using an enumeration algorithm. However, the algorithm is computationally demanding for constructing good OMARS designs with many factors and runs. To overcome this issue, we propose a construction method for large OMARS designs that concatenates two definitive screening designs. The method ensures the core properties of an OMARS design and improves the good statistical features of its parent designs. The concatenation employs an algorithm that minimizes the aliasing among the second-order effects using foldover techniques and column permutations for one of the parent designs. We study the statistical properties of the new OMARS designs and compare them to alternative designs in the literature. Our method increases the collection of OMARS designs for practical applications.\nEric Schoen\n\nKU Leuven\n\nOrder-Of-Addition Orthogonal Arrays To Study The Effect Of Treatment Ordering\n\nThe sequence in which a set of \\(m\\) treatments is applied can be modeled by relative-position factors that indicate whether treatment \\(i\\) is carried out before or after treatment \\(j\\), or by the absolute position for treatment \\(i\\) in the sequence. A design with the same normalized information matrix as the design with all \\(m!\\) sequences is called an order-of-addition orthogonal array. In a recent paper, Peng, Mukerjee and Lin proved that such an array is \\(D\\)- and \\(G\\)-optimal for the main-effects model involving the relative-position factors. We prove that such designs are also \\(I\\)-optimal for this model and \\(D\\)- \\(G\\)- and \\(I\\)-optimal for the first-order model in the absolute-position factors. We propose a methodology for a complete or partial enumeration of non-equivalent order-of-addition orthogonal arrays.\nMario Becerra\n\nKU Leuven\n\nBayesian \\(D\\)- And \\(I\\)-Optimal Designs For Choice Experiments Involving Mixtures And Process Variables\n\nMany products and services can be described as mixtures of ingredients. For example, the ingredients used to make a drink, like mango juice, lime juice, and blackcurrant syrup. Usually, the researchers’ interest is in one or several characteristics of the mixture. In this work, the characteristic of interest is the preference of consumers.\nConsumer preferences can be quantified by using discrete choice experiments in which respondents are asked to choose between sets of alternatives. Choice experiments are well-suited to collect data for quantifying preferences for mixtures of ingredients.\nIn addition to the proportions of ingredients, the preference for a mixture may depend on characteristics other than its composition alone. For example, the ideal cocktail composition may depend on the temperature at which it is served. To cope with this kind of complication, the choice model for mixtures must be extended to deal with the additional characteristics, typically called process variables. In these cases, standard linear regression models are inestimable. Hence, dedicated models are necessary, such as Scheffé models.\nAs experiments in general are expensive and time-consuming, efficient experimental designs are required to provide reliable statistical modeling. Two optimality metrics have usually been studied: \\(D\\)-optimality and \\(I\\)-optimality. The Bayesian version of these metrics is obtained by assigning a prior distribution to the parameters of the model and averaging over the prior. We will show and compare the properties of Bayesian \\(D\\)- and \\(I\\)-optimal designs for choice experiments with mixtures of ingredients and process variables assuming a Scheffé model for utility description.\nSander van Cranenburgh\n\nTU Delft\n\nOn The Impact Of Decision Rule Assumptions In Experimental Designs On Preference Recovery\n\nEfficient experimental designs aim to maximise the information obtained from stated choice data to estimate discrete choice models’ parameters statistically efficiently. Almost without exception, efficient experimental designs are based on the assumption that decision-makers use a Random Utility Maximisation (RUM) decision rule. When using such designs, researchers (implicitly) assume that the decision rule used to generate the efficient design has no impact on respondents’ choice behaviour. However, recent research has called this assumption into question. This study investigates whether the decision rule assumption underlying an experimental design affects respondents’ choice behaviour. To do so, we conducted four stated choice experiments, two based on experimental designs optimised for utility maximisation and the other two based on experimental designs optimised for a mixture of RUM and Random Regret Minimisation (RRM). We evaluate the model fits of RUM and RRM models across the four data sets and investigate whether some choice tasks particularly invoke RUM or RRM decision rules. For the latter, we develop a new sampling-based approach that avoids the confounding between preference and decision rule heterogeneity that plagues latent class-based methods. We find no evidence that RUM-optimised designs particularly invoke RUM-consistent choice behaviour. However, we find compelling evidence that some choice tasks invoke RUM consistent behaviour while others invoke RRM consistent behaviour. This implies that respondents’ choice behaviour and choice modelling outcomes are not exogenous to choice tasks selected by the choice modeller.\nHeiko Großmann\n\nUniversität Magdeburg\n\nDesign Replication In Partial-Profile Choice Experiments\n\nFor design problems in linear models where a finite group of transformations acts transitively on a finite design space, it is well known that for convex optimality criteria which are invariant under the group, optimal approximate designs can be constructed by symmetrizing a given design. The underlying ideas can also be used to address some practical issues which arise in the area of partial-profile discrete choice experiments. In these experiments, there exist potentially many qualitative factors, of which only a subset is used in each question of a choice questionnaire. Certain exact designs for these experiments possess a high efficiency but are “rigid” in the sense that they only use a small number of all possible subsets of the factors. When using such a design as the basis for a survey, where the number of potential respondents would allow several replications of the design, simply repeating the rigid base design does not seem to be advisable. In order to ameliorate this issue, we propose to use replications where each replication of the design uses a different permutation of the factors. For the rigid base designs we consider, this approach leads to replicated designs with a better coverage of the design space and higher statistical efficiency. Moreover, the replicated designs appear to be robust against efficiency losses due to non-response. We illustrate the general ideas by referring to an example from an actual choice experiment.\nJon Gillard\n\nUniversity of Cardiff\n\nDistance In Big Dimensions\n\nThe ‘gold-standard’ distance measure in multivariate statistics is the Mahalanobis distance, but this requires knowledge or computation of an inverse covariance matrix. In big dimensions to compute such an inverse is challenging and anyhow, it is likely to be ill-conditioned. In this talk we introduce and discuss two approaches of quantifying distance which avoid inversion of a covariance matrix: the so-called k-simplicial distances and k-minimal-variance distances. The family of k-simplicial distances includes the Euclidean distance, the Mahalanobis distance, Oja’s simplex distance and many others. We introduce a new family of distances which we call k-minimal-variance distances. Each of these distances are constructed using polynomials in the sample covariance matrix, with the aim of providing an alternative to the inverse covariance matrix, particularly applicable when data is degenerate. We will describe some applications of the considered distances, including outlier detection and clustering.\nBertrand Gauthier\n\nUniversity of Cardiff\n\nSampling And Low-Rank Approximation \n\nIntegral operators with positive-semidefinite (PSD) kernels play a central role in the theory of reproducing kernel Hilbert spaces (RKHSs) and their applications. As an important instance, this class of operators encompasses the PSD matrices. In this talk, we will describe how trace-class integral operators with PSD kernels can be isometrically represented as potentials, or kernel embeddings of measures, in RKHSs with squared-modulus kernels. We will then discuss the connections between the approximation of such operators and the approximation of integral functionals on RKHSs with squared-modulus kernels, and present some sampling-based approximation strategies leveraging the properties of the considered framework.\nLuc Pronzato\n\nUniversité Côte d’Azur\n\nKernel Relaxation For Space-Filling Design}\n\nThe packing radius, covering radius and \\(L_r\\)-quantisation error \\((r>1)\\) of a sampling design \\({\\mathbf X}_n\\) in a compact subset \\({\\mathscr X}\\) of \\({\\mathscr R}^d\\) form natural measures of its space-filling performance in \\({\\mathscr X}\\). They are also key factors for the derivation of error bounds for function approximation or integration over \\({\\mathscr X}\\).Maximising a \\(\\ell_q\\) relaxation of the packing radius is equivalent to minimising the discrete energy for the Riesz kernel \\(K({\\mathbf x},{\\mathbf x}')=\\|{\\mathbf x}-{\\mathbf x}'\\|^{-q}\\), that is, to constructing of a set of \\(q\\)-Fekete points. Other kernels \\(K\\) can also be used, and a continuous version of the problem can be considered: it corresponds to the construction of a minimum-energy probability measure \\(\\mu_K^+\\) on \\({\\mathscr X}\\) for \\(K\\), a problem for which a simple Equivalence Theorem can be formulated.Kernel relaxation can also be introduced in the minimisation of the covering radius, leading to the notion of discrete polarisation. We show that when the minimum-energy probability measure \\(\\mu_K^+\\) has full support \\({\\mathscr X}\\), it is also optimal for the continuous polarisation problem, this support condition being intimately related to the existence of the continuous BLUE in the location problem where \\(K\\) defines the errors correlation.Finally, we also consider a continuous relaxed version of the minimisation of the \\(L_r\\)-quantisation error, and show that the optimal solution is obtained by maximising a concave functional. A necessary and sufficient condition for optimality, i.e., an Equivalence Theorem, is given.\nAnatoly Zhigljavsky\n\nUniversity of Cardiff\n\nOLSE And BLUE For The Location Model And Energy Minimization\n\nIn this talk, we will make connections between the following areas: (a) simple and ordinary kriging with kernel K for prediction of values of a random field indexed by a set X, (b) energy minimization for K, and (c) parameter estimation with the Ordinary Least Squares Estimator (OLSE) and the Best Linear Unbiased Estimator (BLUE) in the location model with observations whose correlation is defined by K.  All there three areas are well-studied in modern literature.  Despite both kriging and designing for correlated observations are well-known and well-studied areas in the DOE community, the effect of replacing the OLSE for the BLUE as parameter estimator in kriging prediction models has not been adequately addressed in the DOE literature. Likewise, there is much research on energy minimization but the implications of this well-researched field on properties of the OLSE and the BLUE in regression models were not much discussed in the DOE literature.  The present paper aims at closing these gaps by fully concentrating on the interplay between the above three areas. We emphasize the special role of the constant function and illustrate our results on examples. At the end of the talk, several conjectures will be formulated.\nKirsten Schorning\n\nTechnische Universität Dortmund\n\n\\(A\\)-Optimal Designs For State Estimation In Networks\n\nWe consider two models for estimating the expected states of nodes in networks where the observations at nodes are given by random states and measurement errors. In the first model, we assume independent successive observations at the nodes and the design question is how often the nodes should be observed to obtain a precise estimation of the expected states. In the second model, all nodes are observed simultaneously and the design question is to determine the nodes which need larger precision of the measurements than other nodes. Both models lead to the same design problem. We derive explicitly A-optimal designs for the most simple network with star configuration. Moreover, we consider the network with wheel configuration and derive some conditions which simplify the numerical calculation of the corresponding \\(A\\)- optimal designs.\nAlen Alexanderian\n\nNorth Carolina State University\n\nOptimal Experimental Design For Infinite-Dimensional Bayesian Inverse\n\nWe consider optimal experimental design for infinite-dimensional Bayesian\ninverse problems governed by PDEs that contain secondary model uncertainties, in\naddition to the uncertainty in the inversion parameters. Our focus will be\nmainly on linear inverse problems with reducible secondary model uncertainties;\nthese are parametric uncertainties that can be reduced through parameter\ninference. For such problems, we seek experimental designs that minimize the\nposterior uncertainty in the inversion parameters, while accounting for the\nuncertainty in secondary parameters. To this end, we derive a marginalized\n\\(A\\)-optimality criterion and develop an efficient computational approach for its\noptimization. We illustrate our approach for estimating an uncertain\ntime-dependent source in a contaminant transport model with an uncertain initial\nstate as secondary uncertainty. We will also discuss extensions to the cases of\nnonlinear inverse problems and irreducible modeling uncertainties.\nKarina Koval\n\nUniversität Heidelberg\n\nProblems Under Model Uncertainty\n\nSolving the Bayesian optimal experimental design (BOED) problem often involves optimizing an expectation of a utility function or optimality criterion. For Bayesian inverse problems characterized by non-Gaussian posteriors, a closed-form expression for the criterion is typically unavailable. Thus, access to a computationally efficient approximation is crucial for numerical solution of the optimal design problem. We present a flexible sample-based approach for approximating the expected utility function and solving the BOED problem that is based on transportation of measures. Our discussion is supplemented with a numerical example for optimal sensor placement.\nDariusz Uciński\n\nUniversity of Zielona Góra \n\nOptimal Sensor Location For Spatiotemporal Processes And Networks\n\nBest sensor selection is of paramount importance when monitoring spatiotemporal phenomena using large-scale sensor networks. A technique of this type is developed for maximizing the parameter estimation accuracy when the system in question is modelled by a partial differential equation and the measurement noise is correlated. The weighted least squares method is supposed to be used for estimation and the trace of the covariance matrix of the resulting estimator is adopted as the measure of estimation accuracy. This design criterion is to be minimized by choosing a set of spatiotemporal measurement locations from among a given finite set of candidate locations. To make this combinatorial problem computationally tractable, its relaxed formulation is considered. The pivotal role here is played by the decomposition of the covariance kernel matrix into the sum of a positive matrix and a scalar multiple of the identity matrix. Optimal solutions are found using efficient simplicial decomposition which alternates between updating the design weights using a version of the multiplicative algorithm and computing a closed-form solution to a simple linear programming problem. The sequence of iterates monotonically decreases the value of the original convex design criterion. As the resulting relaxed solution is a measure on the set of candidate measurements and not a specific subset, randomization and a restricted exchange algorithm are used to convert it to a nearly-optimal subset of selected sensors. A simulation experiment is reported to validate the proposed approach.\nDavid Ginsbourger\n\nUniversity of Bern\n\nOn Gaussian Process Multiple-Fold Cross-Validation\n\nIn this talk I will give an overview of some recent results pertaining to Gaussian Process multiple-fold cross-validation.\nIn the first part, the focus will be put on results from (arXiv:2101.03108, joint work with Cedric Schärer), where block inversion is employed to efficiently calculate multiple-fold cross-validation residuals and their covariances. I will discuss implications of the resulting cross-validation covariance structure on model diagnostics and also how its knowledge helps clarifying some connections between cross-validation-based parameter estimation and MLE.\nIn the second part I will focus on the impact of grouping the observations, referred to as “fold design”, on the estimation of kernel hyperparameters. In particular, numerical results from a joint work with Athénaïs Gautier and Cédric Travelletti on an inverse problem from geosciences will be used to illustrate how the way of designing folds may affect the estimation of a correlation length of interest.\nOverall, presented results enable fast multiple-fold cross-validation, have direct consequences in GP model diagnostics, and pave the way to future work on hyperparameter fitting as well as on the promising field of goal-oriented fold design.\nSelin Ahipasaoglu\n\nUniversity of Southampton\n\nA Modified Frank–Wolfe Algorithm For The Dk-Optimal Design Problem\n\nWe study a first-order method to find the minimum cross-sectional area ellipsoidal cylinder containing a finite set of points. This problem arises in optimal design in statistics when one is interested in a subset of the parameters and referred to as the Dk-optimal design. We provide convex formulations of this problem and its dual, and analyze a method based on the Frank–Wolfe algorithm for their solution. Under suitable conditions on the behaviour of the method, we establish global and local convergence properties. However, difficulties may arise when a certain submatrix loses rank, and we describe a technique for dealing with this situation. \nSamuel Rosa\n\nComenius University Bratislava\n\nMixed-Integer Linear Programming For Computing Optimal Designs\n\nBecause the optimal exact design problem is a discrete optimization problem, it can be solved by general methods of integer programming. One approach to speeding up the solution is to use specialized solvers, provided that a more structured formulation of the problem is known. For instance, for many design criteria a mixed-integer second-order cone programming formulation of the optimal exact design problem was proposed in Sagnol & Harman (2015). We show (see Harman & Rosa, 2023) that for some criteria this mathematical programming specialization can be taken even further. In particular, we provide a mixed-integer linear programming (MILP) formulation for optimal replication-free designs with respect to a wide class of criteria, including \\(A\\)-, \\(I\\)-, \\(G\\)- and \\(MV\\)-optimality. We also show that the MILP formulation can be extended to exact designs with replications and demonstrate some unique advantages of the MILP approach.\nAndrey Pepelyshev\n\nUniversity of Cardiff\n\nPrediction In Regression Models With Continuous Observations\n\nWe consider the problem of predicting  values of a random process or field  satisfying  a linear  model \\(y(x)=\\theta^\\top f(x) + \\varepsilon(x)\\), where  errors \\(\\varepsilon(x)\\) are correlated. This is a common problem  in\nkriging, where the case of discrete observations is standard. By  focussing on the case of continuous observations, we  derive expressions for the best linear unbiased predictors and their mean squared error.\nOur  results are also applicable in the case where the derivatives of the process \\(y\\) are  available,\nand either a response or one of its  derivatives need to be  predicted.  The theoretical results are illustrated\nby several  examples in particular for the popular Matérn  \\(3/2\\) kernel.\nMatthew Hutchings\n\nUniversity of Cardiff\n\nEnergy-Based Sampling For PSD-Matrix Approximation\n\nThe low-rank approximation of large-scale matrices through column sampling is a core technique in machine learning and scientific computing. As its name suggests, this approach consists in defining a low-rank approximation of a given matrix from a subset of its columns, naturally raising questions related to the characterisation of subsets leading to accurate approximations. In practice, the column sampling problem (CSP) is made difficult by its combinatorial nature and by the cost inherent to the assessment of the approximation errors. In this talk, we will present a pseudoconvex differentiable relaxation of the CSP for positive-semidefinite (PSD) matrices. The considered relaxation is based on the isometric representation of weighted PSD matrices as potentials, significantly reducing the numerical cost related to the exploration of the underlying approximation landscape. We will describe some sampling strategies which leverage the proposed relaxation, and illustrate their behaviour on a series of examples. The considered framework can be extended to kernel-matrix and integral-operator approximation, and is intrinsically related to the optimal design of experiments in second-order random field models.\n\n\n\n\n\n\n\n\n\n© 2023 mODa board. Contact: D.Woods@soton.ac.uk\n\n\n\n",
      "last_modified": "2023-06-15T21:31:08+01:00"
    },
    {
      "path": "programme.html",
      "title": "mODa13",
      "description": "<b>Programme<\/b>",
      "author": [],
      "contents": "\nAn outline schedule for the workshop is given below. More details will be available soon.\nThe workshop will start with registration and a welcome reception/buffet on Sunday evening (9 July; 18.00 - 20.00).\n\n\nPreliminary schedule\n    \n      Monday\n      Tuesday\n      Wednesday\n      Thursday\n      Friday\n    08.50 - 09.00\nWelcome\n\n\n\n09.00 - 10.30\nTalks\nTalks\nTalks\nTalks\nTalks10.30 - 11.00\nCoffee\nCoffee\nCoffee\nCoffee\nCoffee11.00 - 12.30\nTalks\nTalks\nTalks\nTalks\nTalks12.30 - 14.00\nLunch\nLunch\nLunch\nLunch\nLunch14.00 - 15.30\nTalks\nPosters\nExcursion\nTalks\n15.30 - 16.00\nTea\nTea\n\nTea\n16.00 - 17.30\nTalks\nTalks\n\nTalks\n\n\n\n\n\n",
      "last_modified": "2023-06-15T21:31:09+01:00"
    },
    {
      "path": "registration.html",
      "title": "mODa13",
      "description": "<b>Registration<\/b>",
      "author": [],
      "contents": "\nDetails about registration have been circulated via email.\n\n\n\n",
      "last_modified": "2023-06-15T21:31:10+01:00"
    },
    {
      "path": "timetable.html",
      "title": "mODa13",
      "description": "<b>Timetable<\/b>",
      "author": [],
      "contents": "\nWorkshop:\nPre-registration: March 2023 (participants will be contacted by email)\nWorkshop fee payment and abstract submission: 30 May\nmODa13: 9-14 July 2023\nSpecial issue:\nFull paper submission deadline: 31 December 2022\nDeadline for first reviews: 31 January 2023\nAcceptance notification: 10 March 2023\nCamera-ready versions: 17 March 2023\n\n\n\n",
      "last_modified": "2023-06-15T21:31:10+01:00"
    },
    {
      "path": "travel.html",
      "title": "mODa13",
      "description": "<b>Travel<\/b>",
      "author": [],
      "contents": "\nParticipants should travel to Chamberlain Hall for accomodation check-in and, on Sunday evening (9 July), workshop registration and the welcome reception.\n\n\nTrain\nThe closest station is Southampton Airport Parkway which is on the mainline from London. If you are arriving via Gatwick, there are services instead into Southampton Central. From either station, you can get the U1 bus to Highfield Campus (U1C from Parkway, U1A from Central), which is a short walk to Chamberlain Hall. From Southampton Central, you can get the U2B bus direct to Chamberlain. Taxis are also available from both stations (there tend to be more at Central).\nAirports\nFrom London Heathrow, the most convenient connection to Southampton is via the National Express Coach. Direct coaches take about two hours and stop on Burgess Road next to Highfield Campus. From London Gatwick, there are Southern Railway services to Southampton Central (see above). Please be aware that engineering work quite often takes places on a Sunday and may disrupt your journey.\nSouthampton Airport is situated on the edge of the city, and connected to Highfield Campus by the U1C bus. It has flights from many major UK cities, Amsterdam and Paris.\n\n\n\n",
      "last_modified": "2023-06-15T21:31:11+01:00"
    }
  ],
  "collections": []
}
